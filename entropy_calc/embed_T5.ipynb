{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36c74ff-4175-4e30-bde1-224cba5d0bfa",
   "metadata": {},
   "source": [
    "# Embed Sequences through protT5MLM and evotuned versions\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5128e188-62f8-46fb-912c-05cebc367a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutput, MaskedLMOutput\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from Bio import SeqIO\n",
    "from datasets import Dataset\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, Seq\n",
    "import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3722c3c0-22d7-49b6-8d51-347fd68f249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5LMHead(nn.Module):\n",
    "    \"\"\"Head for masked language modeling. Linear -> Gelu -> Norm -> Linear + Bias\n",
    "    Outputs logits the size of the vocabulary (128)\n",
    "    Adapted from ESMForMaskedLM\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.d_model, config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "        self.decoder = nn.Linear(config.d_model, 128, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(128))\n",
    "\n",
    "    @staticmethod\n",
    "    def gelu(x):\n",
    "        \"\"\"\n",
    "        This is the gelu implementation from the original ESM repo. Using F.gelu yields subtly wrong results.\n",
    "        \"\"\"\n",
    "        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.decoder(x) + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd93372-acc5-4b90-92f6-096eb7cdda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5EncoderMLM(T5EncoderModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.custom_lm_head = T5LMHead(\n",
    "            config\n",
    "        ) \n",
    "        self.init_weights()\n",
    "        print(config)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        factor = (\n",
    "            self.config.initializer_factor\n",
    "        )  # Used for testing weights initialization\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n",
    "            )\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, T5LMHead):\n",
    "            module.dense.weight.data.normal_(\n",
    "                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n",
    "            )\n",
    "            module.dense.bias.data.zero_()\n",
    "            module.layer_norm.weight.data.fill_(1.0)\n",
    "            module.layer_norm.bias.data.zero_()\n",
    "            module.decoder.weight.data.normal_(\n",
    "                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n",
    "            )\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], MaskedLMOutput]:\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        lm_logits = self.custom_lm_head(encoder_outputs[0])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            labels = labels.to(\n",
    "                lm_logits.device\n",
    "            )  # ensure logits and labels are on same device\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + encoder_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4911c306-8fd2-4bf7-9780-d6efafd6c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer = T5Tokenizer.from_pretrained(\n",
    "    \"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False\n",
    ")\n",
    "\n",
    "# Add masking token to the tokenizer for the datacollator to use:\n",
    "tokenizer.add_special_tokens({\"mask_token\": \"<mask>\"})\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, return_tensors=\"pt\", mlm_probability=0.15\n",
    ")  # provide random masking and return tensors during training per-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc38e06-909f-4c3b-b98c-13232aa1b0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0280e0a4-02c0-49dc-92cf-3c0b3d803f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embedall_n_entropy_T5(modnam, indf, outf):\n",
    "    \n",
    "    #make df with all probabilities\n",
    "    allentropies = []\n",
    "\n",
    "    #store the hidden states\n",
    "    allstates = {}\n",
    "\n",
    "    #store full logit dfs \n",
    "    alllogitdfs = []\n",
    "\n",
    "    #load model to GPU\n",
    "    mod = T5EncoderMLM.from_pretrained(f\"{modnam}\", ignore_mismatched_sizes=True)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    if torch.cuda.is_available():\n",
    "        mod =  mod.to(device)\n",
    "        print(\"%s transferred model to GPU\"%modnam)  \n",
    "\n",
    "    print(len(indf))\n",
    "    \n",
    "    maxslen = max(list(indf.seqlen))\n",
    "\n",
    "    c=0\n",
    "    stt = time.time()\n",
    "\n",
    "    #parses sequences through the 'nodedetails' input df\n",
    "    for seqid in list(indf.node):\n",
    "\n",
    "        #time\n",
    "        c=c+1\n",
    "        if c%100==0:\n",
    "            print(c)\n",
    "            print((time.time() - stt)/60, 'min')\n",
    "\n",
    "        #prepare sequence for embedding\n",
    "        seq = str( list(indf[indf.node == seqid].seq)[0] ).replace('J', 'X')\n",
    "\n",
    "        formseq = [\" \".join(list(seq))]\n",
    "\n",
    "        token_encoding = tokenizer(formseq, add_special_tokens=True, padding=\"longest\")\n",
    "        input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "\n",
    "        #embed\n",
    "        if torch.cuda.is_available():\n",
    "            m_results = mod(input_ids, attention_mask=attention_mask, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "        #softmax to normalise the logit values for this sequence\n",
    "        m_logits = torch.nn.LogSoftmax(dim=1)((m_results[\"logits\"][0]).to(device=\"cpu\")).detach()\n",
    "\n",
    "        #average out position states for each dimension/layer (for 24 layer model) -> one number per model dimension per layer\n",
    "        m_hstates = [np.mean(m_results['hidden_states'][layernum][0].to(device=\"cpu\").detach().numpy(), axis = 0) for layernum in range(0,25)]\n",
    "        \n",
    "        #make dataframe with logits\n",
    "        df = pd.DataFrame(m_logits)\n",
    "        #add all model tokens as columns\n",
    "        df.columns = [x.replace(\"â–\", \"\") for x in list(tokenizer.get_vocab())[:-1]]\n",
    "        \n",
    "        #make positions column\n",
    "        df[\"pos\"] = df.index.values\n",
    "        df = df.melt(id_vars=\"pos\").sort_values([\"pos\",\"variable\"])\n",
    "        #get probabilities by exp the softmaxed values\n",
    "        df[\"probability\"] = np.exp(df.value)\n",
    "        real_amino_acids = [\"A\",\"R\",\"N\",\"D\",\"C\",\"Q\",\"E\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"Y\",\"V\"]\n",
    "        #only keep amino acid tokens\n",
    "        df = df[df.variable.isin(real_amino_acids)]\n",
    "        max_probs = [sum(df[df.pos == pos].probability) for pos in df.pos.sort_values().unique()]\n",
    "        #normalise by maximum prob in each position to get adjusted probability (token probs within one position sum up to 1)\n",
    "        df[\"token_adjusted_probability\"] = [max_probs[pos] for pos in df.pos]\n",
    "        df[\"token_adjusted_probability\"] = df[\"probability\"]/df[\"token_adjusted_probability\"]\n",
    "        #remove positions that match to start/end special tokens\n",
    "        df = df[(df.pos>=1)]\n",
    "\n",
    "        #list of base 2 entropies of token adjusted probs across aa tokens for each position\n",
    "        embentropies = [scipy.stats.entropy(list(df[(df.pos == i+1)]['token_adjusted_probability']), base=2) for i in range(len(seq))]\n",
    "\n",
    "        #add padding if the sequence is shorter than the longest of the alignment\n",
    "        if len(embentropies) < maxslen:\n",
    "            embentropies = embentropies + ['' for x in range(maxslen - len(embentropies))]\n",
    "\n",
    "        allentropies.append([seqid] + embentropies)\n",
    "\n",
    "        allstates.update({seqid:m_hstates})\n",
    "\n",
    "        df['seqid'] = [seqid for i in range(len(df))]\n",
    "\n",
    "        alllogitdfs.append(df)\n",
    "\n",
    "    \n",
    "\n",
    "    #turn entropy lists into df\n",
    "    entrodf = pd.DataFrame(allentropies)\n",
    "    # entrodf.columns = ['name'] + [i+1 for i in range(len(seq))]\n",
    "    entrodf.columns = ['name'] + [i+1 for i in range(maxslen)]\n",
    "\n",
    "    entrodf.to_csv(outf + '-site_entropy.csv', index=False)\n",
    "\n",
    "    #export hidden states as pickle\n",
    "    with open(outf + '.pickle', 'wb') as out:\n",
    "        pickle.dump(allstates, out, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #export df with all logit probs\n",
    "    fnlogitdf = pd.concat(alllogitdfs)\n",
    "    fnlogitdf.to_csv(outf + '-all_logitprobs.csv', index=False)\n",
    "    \n",
    "    \n",
    "    return entrodf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f0fca-2792-4d86-b3cd-92eb72965e20",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f1d033-9d63-4735-9510-ae85112c5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example for H7 seqs\n",
    "\n",
    "h7asrseqs = pd.read_csv('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt_nodedetails.csv')\n",
    "\n",
    "\n",
    "\n",
    "#single serotypes\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_uniref_ncbi_e10_ncbiflu_HA_all_110424_noX_clu99_filt_H1/trainer/checkpoint-1000',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_uniref_H1_221124')\n",
    "\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_uniref_ncbi_e10_ncbiflu_HA_all_110424_noX_clu99_filt_H5/trainer/checkpoint-344',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_uniref_H5_221124')\n",
    "\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_uniref_ncbi_e10_ncbiflu_HA_all_110424_noX_clu99_filt_H7/trainer/checkpoint-112',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_uniref_H7_221124')\n",
    "\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_uniref_ncbi_e10_ncbiflu_HA_all_110424_noX_clu99_filt_H3/trainer/checkpoint-1860/',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_uniref_H3_221124')\n",
    "\n",
    "\n",
    "#HA-80\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_uniref_ncbi_e10_ncbiflu_HA_all_110424_noX_clu99_filt_datesplit8020/trainer/checkpoint-3550/',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_uniref_8020_221124')\n",
    "\n",
    "\n",
    "#base\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_UniRef_2e_constant_20241106/trainer/checkpoint-127292',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_UniRef_e2')\n",
    "\n",
    "\n",
    "#HA-all\n",
    "embedall_n_entropy_T5('/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/ncbiflu_protT5_221124/T5EncoderMLM_uniref_ncbi_e10_ncbiflu_HA_all_110424_noX_clu99_filt_all/trainer/checkpoint-3897',\n",
    "                      h7asrseqs,\n",
    "                      '/media/spyros/HD-ADU3/spyros/flu_LLM_evol_data/gisaid_h7_270624_filt/gisaid_h7_270624_filt-ASR-T5_uniref_HA_221124')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
